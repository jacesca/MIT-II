[Prof. Stefanie Jegelka] 20:11:00
What would be the next? Configuration of that system so what the way this can be done is that we take the system at the current train step.

[Prof. Stefanie Jegelka] 20:11:00
So we get all these particles. And we build a graph where we connect the nearby particles.

[Prof. Stefanie Jegelka] 20:11:05
So this is called a nearest neighbor graph. Then we can compute a representation with a graph Neal network.

[Prof. Stefanie Jegelka] 20:11:13
We can basically this is us going to exchange messages between them and update their information.

[Prof. Stefanie Jegelka] 20:11:20
And then we can extract basically variable components. So this is using a graph neural network to run the simulation essentially.

[Prof. Stefanie Jegelka] 20:11:34
Now, here's an example that it may be more familiar to many of you.

[Prof. Stefanie Jegelka] 20:11:39
And this is Google Maps. So what happens? There is okay.

[Prof. Stefanie Jegelka] 20:11:42
Well, today, I wanna go to work. How long would it take me to get there?

[Prof. Stefanie Jegelka] 20:11:47
On different routes. So what it does is, it gives me an estimated time of arrival, and that estimated time of arrival is computed with a graph neural network, so.

[Prof. Stefanie Jegelka] 20:12:01
The way this is done is, we have our street network, and we chopped it up.

[Prof. Stefanie Jegelka] 20:12:06
Basically the streets into some segments, and then we encode these segments as graphs.

[Prof. Stefanie Jegelka] 20:12:11
We are obviously like the notes are locations, and then you put an edge wherever like the locations are next to each other, they are connected with the street, and then you use that graph neural network to predict the travel times and once you have the travel times then you can it just compute how long it

[Prof. Stefanie Jegelka] 20:12:32
will take, and then you can do like the round ranking, etc.

[Prof. Stefanie Jegelka] 20:12:36
But the actual travel type prediction is done with a graph neural network.

[Prof. Stefanie Jegelka] 20:12:45
And there's many, many more applications where we have data that can be encoded as a graph.

[Prof. Stefanie Jegelka] 20:12:49
And we want to make a prediction from that graph.

[Prof. Stefanie Jegelka] 20:12:53
So I hope I convinced you now that this is actually relevant in many different areas.

[Prof. Stefanie Jegelka] 20:12:59
Let me now explain to you a little bit how we could build a new network that takes a graph as an input.

[Prof. Stefanie Jegelka] 20:13:09
And outputs a prediction either for notes in the graph or for the full graph itself.

[Prof. Stefanie Jegelka] 20:13:16
So if we think about it, what should this neural network do?

[Prof. Stefanie Jegelka] 20:13:22
It should encode any kind of information we have about any of the notes in the graph or the edges in the Graph.

[Prof. Stefanie Jegelka] 20:13:43
So all the side information, like in the molecule level type information.

[Prof. Stefanie Jegelka] 20:13:43
These are the different atoms. What are the atom types, etc.

[Prof. Stefanie Jegelka] 20:13:43
In the social network. I have some information about the users, and it was a code information about the structure of the graph, like who's connected to who in a molecule, what we want is like some other information about the configuration, etc., so that's essentially what it what's likely predictive

[Prof. Stefanie Jegelka] 20:13:58
that's what we wanted to capture. And the question is, how could we capture that?

[Prof. Stefanie Jegelka] 20:14:13
And before we go there, let me go back to images. Why am I going back to images?

[Prof. Stefanie Jegelka] 20:14:19
Because you could think of an image as a graph as well.

[Prof. Stefanie Jegelka] 20:14:24
An image has pixels, and so you could just call the pixels that are next to each other.

[Prof. Stefanie Jegelka] 20:14:30
So you have a graph and now, if we look at the Cnn the convolutional network what it does is it looks at local poweratches in this image.

[Prof. Stefanie Jegelka] 20:14:39
It that was my sliding window, as you remember, so you could think about it.

[Prof. Stefanie Jegelka] 20:14:44
It's just looking at sort of a local neighborhood of notes in of a note in the graph so the note center, like where the filter is set.

[Prof. Stefanie Jegelka] 20:14:53
Now, what could be maybe transfer that ideal to other really old graphs that are not images, and in fact, we'll use the same idea of encoding local neighborhoods.

[Prof. Stefanie Jegelka] 20:15:09
And there's now only a few differences. So what's a local neighborhood?

[Prof. Stefanie Jegelka] 20:15:14
A local neighbor. It is basically, if I look at a node, look how which are the notes I can reach within one hop.

[Prof. Stefanie Jegelka] 20:15:21
That's the direct neighbors, or maybe 2 hop cycle who hops from here to here, and so on.

[Prof. Stefanie Jegelka] 20:15:27
So something like that that's called a local neighborhood.

[Prof. Stefanie Jegelka] 20:15:31
No, there's some important difference in that in the Cnn.

[Prof. Stefanie Jegelka] 20:15:36
This patch, all this has in this case a 9 pixel in it.

[Prof. Stefanie Jegelka] 20:15:41
Here, Lord may have a lot of friends, or very few friends, if I have a lot of friends, my local patch will be much larger.

[Prof. Stefanie Jegelka] 20:15:50
It has many more people in it than if I have only a few friends.

[Prof. Stefanie Jegelka] 20:15:54
So this guy over here has maybe few friends than like someone in the middle who have, like a larger network around.

[Prof. Stefanie Jegelka] 20:16:02
So we cannot directly use the idea of that convolution with the patches.

[Prof. Stefanie Jegelka] 20:16:07
But we'll use some veryation of it.

[Prof. Stefanie Jegelka] 20:16:13
So let me first give you the big picture of it. So what we are going to do is we are first going to compute a node, encoding or anode embedding where we are encoding this local neighborhood of a who are they connected to who are their friends connected to something like that what's the

[Prof. Stefanie Jegelka] 20:16:31
structure of this? What are the properties of the friends and all that stuff sits in the enquiry for that particular month neighborhood, invite? We do this for?

[Prof. Stefanie Jegelka] 20:16:43
So we get one vector that captures all that information. And when we do this for all the nodes in the network.

[Prof. Stefanie Jegelka] 20:16:49
So we get one vector that in codes, the information about that neighborhood.

[Prof. Stefanie Jegelka] 20:16:54
So we get that collection of facts. Now, if we want to make a prediction about a node, we can just use that vector in a classified that's here is the encoding part.

[Prof. Stefanie Jegelka] 20:17:07
And then we just use the put the classifier on top, just like we did with the other.

[Prof. Stefanie Jegelka] 20:17:13
The other thing we could do is if we want a prediction on the full graph, we have to somehow capture the information from all these vectors and put it into a single encoding for that graph.

[Prof. Stefanie Jegelka] 20:17:29
So that's the so called readout. So we'll just take that collection of factors and we'll compress it into a single vector there's various ways you can do this.

[Prof. Stefanie Jegelka] 20:17:42
You can take a sound, you can take an average something like that.

[Prof. Stefanie Jegelka] 20:17:46
That's what essentially is being done. You could do some normally linear transformation of each of the vectors.

[Prof. Stefanie Jegelka] 20:17:53
And then just take an average or so so that's the readout information and the readout operation.

[Prof. Stefanie Jegelka] 20:17:59
And then we have one vector that in codes, the entire graph.

[Prof. Stefanie Jegelka] 20:18:04
So if I want, like, say, to make a prediction about a molecule, I would do that, and then use that last grave.

[Prof. Stefanie Jegelka] 20:18:11
Vector and you set in a classifier to make a prediction like a classification prediction on that monic.

[Prof. Stefanie Jegelka] 20:18:19
What I haven't explained to you yet is, how do I actually encode these note neighborhoods?

[Prof. Stefanie Jegelka] 20:18:26
So I bet it should include a whole lot of information. But how do I actually do that?

[Prof. Stefanie Jegelka] 20:18:39
And this is usually done with message passing. Let me explain to you this a little bit more.

[Prof. Stefanie Jegelka] 20:18:46
So here's a part of my graph, this actually an illustration from Google from that.

[Prof. Stefanie Jegelka] 20:18:51
Google Maps description and let's look at the local neighborhood.

[Prof. Stefanie Jegelka] 20:18:58
And what we'll do is we will send messages between the nodes.

[Prof. Stefanie Jegelka] 20:19:04
So each node will have and update its representation.

[Prof. Stefanie Jegelka] 20:19:09
It's encoding, and they will share that with its neighbors, and a lord will use.

[Prof. Stefanie Jegelka] 20:19:14
Its neighbors encodings to update its own recording.

[Prof. Stefanie Jegelka] 20:19:19
And that way, you're basically including the neighborhood.

[Prof. Stefanie Jegelka] 20:19:26
And the way this is done is that you can really think of each node as a little processing.

[Prof. Stefanie Jegelka] 20:19:30
Unit. So within each notes, it's a little neural network, and you get the neighbors information.

[Prof. Stefanie Jegelka] 20:19:40
And it's processed via this neural network.

[Prof. Stefanie Jegelka] 20:19:43
And then that helps the normal like update its own representation.

[Prof. Stefanie Jegelka] 20:19:48
That's how this is done. So you sort of have a little neural network sitting in each node.

[Prof. Stefanie Jegelka] 20:19:54
And it's actually the same neural network. This is a again the weight sharing that we talked about on Vince. They remember that the say I use the same detector on each image patch.

[Prof. Stefanie Jegelka] 20:20:04
I use the same way to process the neighbors information in each node.

[Prof. Stefanie Jegelka] 20:20:09
It's the same idea that way. I have to learn this, you know, network only once, and I just reuse it.

[Prof. Stefanie Jegelka] 20:20:16
And that saves me a whole lot of parameters.

[Prof. Stefanie Jegelka] 20:20:21
Alright. So let's take a little deeper into what this could look like.

[Prof. Stefanie Jegelka] 20:20:26
So here's my center note, and these are the neighbors, and each of them has its vector that's the description or representation of that node.

[Prof. Stefanie Jegelka] 20:20:35
So now we go, and round. We pre specify the number of rounds ahead of time.

[Prof. Stefanie Jegelka] 20:20:41
So that's part of my architecture. And then in each round each note in parallel gaps, the representations of its neighbors.

[Prof. Stefanie Jegelka] 20:20:51
So these are the age here. That's the representation of the neighbor.

[Prof. Stefanie Jegelka] 20:20:55
And then it compresses them into a single vector. So this function here, that's my neural network.

[Prof. Stefanie Jegelka] 20:21:01
It. What it does is it takes the neighbors information, and it up.

[Prof. Stefanie Jegelka] 20:21:08
A, compressed it into a single vector and then I use that call the message.

[Prof. Stefanie Jegelka] 20:21:14
I use that message to update the my own representation, so that red guys in representation.

[Prof. Stefanie Jegelka] 20:21:24
So this could just be awaited from.

[Prof. Stefanie Jegelka] 20:21:29
And once I do this over a few routes, you see, you actually collect information from a larger and larger neighborhood.

[Prof. Stefanie Jegelka] 20:21:36
So in the first round I get information only from my friends.

[Prof. Stefanie Jegelka] 20:21:41
My direct friends, but in parallel. My friends are collecting information from their friends.

[Prof. Stefanie Jegelka] 20:21:48
And they quoting it, and then in the next round, I'm asking my friends again, hey?

[Prof. Stefanie Jegelka] 20:21:52
Can you send me your information? They are sending me their representations, and that includes now their friends information.

[Prof. Stefanie Jegelka] 20:22:00
So now I already have information from a tool of neighborhood, and if I go in 10 rounds I have for information from a much larger neighborhood.

[Prof. Stefanie Jegelka] 20:22:07
So you know, each round here sort of the information I'm recording.

[Prof. Stefanie Jegelka] 20:22:11
It's a larger and larger neighborhood radius. Essentially.

[Prof. Stefanie Jegelka] 20:22:18
Okay. So let's look a little bit deeper into what this aggregation function actually could look like.

[Prof. Stefanie Jegelka] 20:22:25
So this is the sort of the heart, the core part of that graph neural network that's a very important operation.

[Prof. Stefanie Jegelka] 20:22:34
And so this is actually one form of neural network or layer that takes each.

[Prof. Stefanie Jegelka] 20:22:41
They takes a set of neighborhood neighbors, and it aggregates them, and I I refer to the transformer architecture earlier.

[Prof. Stefanie Jegelka] 20:22:52
That's exactly what the transformer also does is this, takes a set of tokens, and it aggregates them.

[Prof. Stefanie Jegelka] 20:22:58
That's the same idea. So.

[Prof. Stefanie Jegelka] 20:23:03
What could this aggregation look like if you think about it?

[Prof. Stefanie Jegelka] 20:23:06
It's a function that takes a set of vectors and it I'll quote the single.

[Prof. Stefanie Jegelka] 20:23:12
Vector so the simplest way to do this would be to average them or sum them up.

[Prof. Stefanie Jegelka] 20:23:17
That's the simplest way doesn't need any learning.

[Prof. Stefanie Jegelka] 20:23:23
I can just do this, but that would be a simple way of doing it likewise.

[Prof. Stefanie Jegelka] 20:23:30
I could do a coordinate, vise, Min or Mac. So what that would mean is if I had, say, 2 vectors as input they have in like coordinates.

[Prof. Stefanie Jegelka] 20:23:40
1 0 and 2, and maybe 0 minus one and.

[Prof. Stefanie Jegelka] 20:23:46
Bye, then the coordinate rights. Max of this would be basically for each coordinate.

[Prof. Stefanie Jegelka] 20:23:55
I look at the larger coordinate of the tool, so it would be one X.

[Prof. Stefanie Jegelka] 20:23:59
It would be 0, and it would be.

[Prof. Stefanie Jegelka] 20:24:02
Why would I want to do something like this? This is actually useful.

[Prof. Stefanie Jegelka] 20:24:06
If all you care about is sort of an extreme over your neighbor.

[Prof. Stefanie Jegelka] 20:24:12
So one place where this is useful. Is, if you want to do path.

[Prof. Stefanie Jegelka] 20:24:15
Finding. So I want to find the shortest route from myself to somewhere.

[Prof. Stefanie Jegelka] 20:24:20
I'll actually see one of my neighbors press the best, so I only care about the neighbor with the best cap, with the shortest distance.

[Prof. Stefanie Jegelka] 20:24:29
Essentially so. I would only care about them in. I don't care about the others.

[Prof. Stefanie Jegelka] 20:24:33
So in that case I would actually do something like the middle of the Max.

[Prof. Stefanie Jegelka] 20:24:38
So, graph neural networks have also been used to learn graph optimization algorithms.

[Prof. Stefanie Jegelka] 20:24:44
And that's where this min and Max aggregation function occur along.

[Prof. Stefanie Jegelka] 20:24:53
And now both of these first ones don't actually have any parameters in them.

[Prof. Stefanie Jegelka] 20:24:59
But there's nothing I would be learning about them. It's just there.

[Prof. Stefanie Jegelka] 20:25:02
So the most general form of this aggregation is the following, and it's essentially a generalization of this averaging.

[Prof. Stefanie Jegelka] 20:25:11
So what you do is to each neighbor you apply a nonlinear function.

[Prof. Stefanie Jegelka] 20:25:19
So this Mlp is basically a fully connected neural network like Monday's lecture.

[Prof. Stefanie Jegelka] 20:25:28
And likewise the other one here. So what you do is you take each of the neighbor vectors individually.

[Prof. Stefanie Jegelka] 20:25:35
You do a linear transformation of them, and then you sum them up, and you may have another Mlp.

[Prof. Stefanie Jegelka] 20:25:43
There, or maybe just something like a real war. So that's also possible.

[Prof. Stefanie Jegelka] 20:25:49
So Mlp is the fully connected neural network now they have parameters that we can learn, and that's essentially the parameters.

[Prof. Stefanie Jegelka] 20:25:56
We learned in the graph.

[Prof. Stefanie Jegelka] 20:25:59
Now, before I go on that we point out, why would you do this?

[Prof. Stefanie Jegelka] 20:26:02
Summing, etc. And why do you apply this neural network to each neighbor individually?

[Prof. Stefanie Jegelka] 20:26:09
And there's an important reason for that. And the important reason is that the aggregation function I'm specified should work.

[Prof. Stefanie Jegelka] 20:26:17
If I have a single neighbor, if I have 2 neighbors, if I have 5 neighbors, if I have 5 neighbors, the same aggregation function, I would learn it once I specify it.

[Prof. Stefanie Jegelka] 20:26:28
Once I learned it's parameters once, and I use it on all the notes, so it better be a function that can deal with different numbers of neighbors.

[Prof. Stefanie Jegelka] 20:26:36
And if I do this, I can apply this to one neighbor.

[Prof. Stefanie Jegelka] 20:26:40
I can apply it to a 100 neighbors. It doesn't matter.

[Prof. Stefanie Jegelka] 20:26:43
It's only that the sum changes. So that's why we use that.

[Prof. Stefanie Jegelka] 20:26:48
Sum in there, and don't, and not something else.

[Prof. Stefanie Jegelka] 20:26:54
So, that's mostly what this aggregation function looks like.

[Prof. Stefanie Jegelka] 20:26:59
And again, this is the little neural network that you saw in the that processing unit that you saw here.

[Prof. Stefanie Jegelka] 20:27:06
This is what it actually looks like, and it has learned about parameters and I'll show you in the second how like that makes it a bit clear how they would be learned.

[Prof. Stefanie Jegelka] 20:27:16
That's the learn about product, that craft, neural network.

[Prof. Stefanie Jegelka] 20:27:20
And then we have the operation. How we combine that message!

[Prof. Stefanie Jegelka] 20:27:24
So this is the kind of compressed information about my neighbors that is what I want to combine with my own representation.

[Prof. Stefanie Jegelka] 20:27:34
And the way I can do this is by a weighted combination, where I learned the weight.

[Prof. Stefanie Jegelka] 20:27:39
So these are weight matrices and this is my own representation.

[Prof. Stefanie Jegelka] 20:27:43
This is the message from the neighbors, and maybe there's an offset term that I would also learn.

[Prof. Stefanie Jegelka] 20:27:49
And these are weight matrices, and then I have a non linearity.

[Prof. Stefanie Jegelka] 20:27:53
So the Sigma would be just a renewal or something like that.

[Prof. Stefanie Jegelka] 20:28:02
So this is the specification of the architectural.

[Prof. Stefanie Jegelka] 20:28:08
And someone is asking Mlp. It's multi-layer perceptron.

[Prof. Stefanie Jegelka] 20:28:12
Yes, that's true. That's what the Mlp.

[Prof. Stefanie Jegelka] 20:28:14
Stands for. That's really a fully connected neural network.

[Prof. Stefanie Jegelka] 20:28:17
The Monday new network.

[Prof. Stefanie Jegelka] 20:28:27
Okay, so now let me just show you a different view of the same thing.

[Prof. Stefanie Jegelka] 20:28:38
So here's my graph, and I want to encode.

[Prof. Stefanie Jegelka] 20:28:42
Say that node a.

[Prof. Stefanie Jegelka] 20:28:46
So well. Note A has neighbors B, C. And D, so not a aggregate.

[Prof. Stefanie Jegelka] 20:28:54
The information from Bcd. And I do this via this aggregation function.

[Prof. Stefanie Jegelka] 20:29:00
So this is that part here, and the combined.

[Prof. Stefanie Jegelka] 20:29:05
So now, what I'm doing is, I'm going to unroll this message.

[Prof. Stefanie Jegelka] 20:29:08
Passing sort of where does the information come from? In the previous iteration?

[Prof. Stefanie Jegelka] 20:29:14
So now I'm going backwards in the iterations.

[Prof. Stefanie Jegelka] 20:29:16
These neighbors collected information from their neighbors, so I can draw that into my picture as well.

[Prof. Stefanie Jegelka] 20:29:21
Here!

[Prof. Stefanie Jegelka] 20:29:23
So B has neighbors A and C, so I just draw them, and they also use that aggregation.

[Prof. Stefanie Jegelka] 20:29:31
This is again the aggregation.

[Prof. Stefanie Jegelka] 20:29:34
And likewise cnd have neighbors, and they aggregate from and let's say we had gone 2 iterations in our message. Passing.

[Prof. Stefanie Jegelka] 20:29:44
This was iteration one. This was iteration. So now this is the representation of a after 2 iterations.

[Prof. Stefanie Jegelka] 20:29:54
The index up there is the number of here. I use the representation of B, and after one iteration, that's one.

[Prof. Stefanie Jegelka] 20:30:03
And this, if the representation of a at iteration 0, that's the input.

[Prof. Stefanie Jegelka] 20:30:11
So now, if you look at it this way, it looks almost like a new network that has layers.

[Prof. Stefanie Jegelka] 20:30:18
It iteration. It's like a layer.

[Prof. Stefanie Jegelka] 20:30:23
So this actually looks like a later.

[Prof. Stefanie Jegelka] 20:30:28
And now with in each layer, I have my aggregation function.

[Prof. Stefanie Jegelka] 20:30:32
So I have sort of a small neural network within each.

[Prof. Stefanie Jegelka] 20:30:36
I within each layer, within each layer.

[Prof. Stefanie Jegelka] 20:30:41
I'm using the same aggregation function. So that is the thing that I said.

[Prof. Stefanie Jegelka] 20:30:44
I specify this once, and I use it on all the notes.

[Prof. Stefanie Jegelka] 20:30:47
This is like in the Cnn. I'm once learning that filter, and I'm using it. That detector and I'm using it on all the patching.

[Prof. Stefanie Jegelka] 20:30:58
So it's the same thing here. Alright! So this looks like layers.

[Prof. Stefanie Jegelka] 20:31:05
And now what this looks like layers like this, you can see.

[Prof. Stefanie Jegelka] 20:31:09
Oh, I could do back propagation through this. This looks just like layers and neural network.

[Prof. Stefanie Jegelka] 20:31:15
So I could actually compute the gradient. So this would be basically from this.

[Prof. Stefanie Jegelka] 20:31:19
Ha! I would do my prediction, and then I have my loss, etc.

[Prof. Stefanie Jegelka] 20:31:25
So once I have that, I can actually do back propagation, I can compute a gradient.

[Prof. Stefanie Jegelka] 20:31:29
I can update my in my way here, and then I can learn this new network.

[Prof. Stefanie Jegelka] 20:31:36
And so for each note I would have this 3 like structure.

[Prof. Stefanie Jegelka] 20:31:41
So, for each node I can gives me a gradient that can do ingredients.

[Prof. Stefanie Jegelka] 20:31:49
So that's how actually, the idea how you can train this this graph neural network.

[Prof. Stefanie Jegelka] 20:31:59
Okay. That we see. There's a few questions. Are the iteration in the Gnn.

[Prof. Stefanie Jegelka] 20:32:06
Also called epochs. Know the iterations because they are kind of like.

[Prof. Stefanie Jegelka] 20:32:10
The iterations of this method, I think an epoch is usually just used for stochastic radio descent, and an epoch means I have once gone through all my data. In some sense.

[Prof. Stefanie Jegelka] 20:32:22
Yeah, here we are going through all the notes in parallel.

[Prof. Stefanie Jegelka] 20:32:26
But this is typically not referred to as an eight-foot.

[Prof. Stefanie Jegelka] 20:32:30
And then like on top of this able to have the iterations of the gradient descent, we are like, I'm updating, based on my data port.

[Prof. Stefanie Jegelka] 20:32:39
But in each gradient descent we would actually compute the gradient by using all the message.

[Prof. Stefanie Jegelka] 20:32:45
Passing steps, because these are the layers in the network.

[Prof. Stefanie Jegelka] 20:32:51
In some sense, does not embedding use parallel processing.

[Prof. Stefanie Jegelka] 20:32:56
That's right. They are actually done in parallel.

[Prof. Stefanie Jegelka] 20:32:58
So you see, they are actually computed in parallel.

[Prof. Stefanie Jegelka] 20:33:01
So in each step I'm computing all the node updates and parallel.

[Prof. Stefanie Jegelka] 20:33:07
How are they initialized? Randomly, just like, have all the other neural networks feed to some random initialization?

[Prof. Stefanie Jegelka] 20:33:16
How do we know that these Pcs. In the second layer?

[Prof. Stefanie Jegelka] 20:33:21
Here are so. The reason these 3 are here is that I'm updating load a and now I'm looking.

[Prof. Stefanie Jegelka] 20:33:26
What are the neighbors of? Note? A. So neighbors of note?

[Prof. Stefanie Jegelka] 20:33:30
A. Is P. And that's where this comes from. So it's really the neighbors.

[Prof. Stefanie Jegelka] 20:33:36
So, who appears there as the in the aggregation.

[Prof. Stefanie Jegelka] 20:33:39
Who's the input to the aggregation depends on the structure of your input. Graph. So the connectivity.

[Prof. Stefanie Jegelka] 20:33:45
Actually depends on the structure of the input graph.

[Prof. Stefanie Jegelka] 20:33:53
How do we choose the note to start the aggregation and combination process this is done in parallel, and the reason is that H.

[Prof. Stefanie Jegelka] 20:34:01
A is a function of the State in the.

[Prof. Stefanie Jegelka] 20:34:07
It's a function of the State in the first step.

[Prof. Stefanie Jegelka] 20:34:12
The second step is to function of the States in the first step.

[Prof. Stefanie Jegelka] 20:34:15
So we computed all this in representations in the first step, then all of the second step representations are functions of the first step.

[Prof. Stefanie Jegelka] 20:34:25
So we have those we can do, those in parallel so that it doesn't.

[Prof. Stefanie Jegelka] 20:34:28
Or if I wanted to do them sequentially, it doesn't matter.

[Prof. Stefanie Jegelka] 20:34:31
It because they age. In the second iteration it only uses the first iteration state, and not the second iteration state.

[Prof. Stefanie Jegelka] 20:34:41
So the order doesn't matter. I've already computed them.

[Prof. Stefanie Jegelka] 20:34:48
Okay, so that is sort of the big picture of what this graphical network looks like.

[Prof. Stefanie Jegelka] 20:34:54
And you see, so it does this like message passing it outputs a recordation.

[Prof. Stefanie Jegelka] 20:34:58
And then we predict, okay, so and then we we can compute the gradient by back propagating through that tree thing that's called a tree like the unrolled thing.

[Prof. Stefanie Jegelka] 20:35:11
So what do we have to still think about? Well, if we do the training, what is a data point?

[Prof. Stefanie Jegelka] 20:35:20
A data point. That depends a bit. What is the task? I have?

[Prof. Stefanie Jegelka] 20:35:24
So if I want to do a note prediction at the node level, one note would be one data point, and then, of course, it would use its neighborhood like to compute the actual representation.

[Prof. Stefanie Jegelka] 20:35:39
But that's one data point. So one graph can give me many data points.

[Prof. Stefanie Jegelka] 20:35:44
If I'm so this would be like the left hand side would maybe be the social network, or where I'm doing link prediction or so.

[Prof. Stefanie Jegelka] 20:35:52
It's a pair of notes. If I have tasks like the molecule prediction, one graph is one data point.

[Prof. Stefanie Jegelka] 20:35:59
So my data set is a data set of a lot of graphs.

[Prof. Stefanie Jegelka] 20:36:02
So typically then I have a data set of lots of molecules. And I'd make a prediction for each of the molecules.

[Prof. Stefanie Jegelka] 20:36:12
And what is the thing I have to specify in the architecture?

[Prof. Stefanie Jegelka] 20:36:17
So I have to specify what form of aggregation I want do I want the one with the neural network?

[Prof. Stefanie Jegelka] 20:36:23
Of the out, essentially there's some few veryations of that.

[Prof. Stefanie Jegelka] 20:36:28
And what is the loss function I want to use? Basically do I do classification or regression?

[Prof. Stefanie Jegelka] 20:36:34
If I do regression, I'd use list squares if I don't.

[Prof. Stefanie Jegelka] 20:36:37
Classification. I use cross entropy just like with the other networks.

[Prof. Stefanie Jegelka] 20:36:44
And then I can just train with stochastic gradient descent.

[Prof. Stefanie Jegelka] 20:36:52
Alright. So this was an architecture of how the message, passing graph neural network, architectural works.

[Prof. Stefanie Jegelka] 20:37:02
So, to summarize, we, what we do is we encode these local neighborhoods, and then we encode them by ring message, passing.

[Prof. Stefanie Jegelka] 20:37:13
And what we actually learn in the graphural network is, how do we aggregate the information from the neighbors?

[Prof. Stefanie Jegelka] 20:37:20
That's the central operation. And then we just pre-specify how often we want to do this.

[Prof. Stefanie Jegelka] 20:37:26
And then we run it. It gives us an encoding for each node, and then we can make predictions with that.

[Prof. Stefanie Jegelka] 20:37:36
There are also toolbox is available. If you actually want to try this out.

[Prof. Stefanie Jegelka] 20:37:41
So, for instance, the Deep Graph library from Amazon or the Pytorch geometric package that you can use, that helps you like get us already models, pre-coded, etc.

[Prof. Stefanie Jegelka] 20:37:58
Alright! So now, in the last few minutes I want to go into an example.

[Prof. Stefanie Jegelka] 20:38:07
Application.

[Prof. Stefanie Jegelka] 20:38:10
Before we do a more general discussion in the. And so this example application, I want to go back to this so-called poly pharmacy example, the one where we want to know if you would take these specific drugs together.

[Prof. Stefanie Jegelka] 20:38:25
What you get specifically, stick or not. What do you get?

[Prof. Stefanie Jegelka] 20:38:29
Some bad side effect. And this is actually not a real problem.

[Prof. Stefanie Jegelka] 20:38:33
So it does occur and it costs the healthcare system a lot of money.

[Prof. Stefanie Jegelka] 20:38:39
So it would be really useful to have a machine learning model model predict that because these are typically difficult to identify.

[Prof. Stefanie Jegelka] 20:38:49
If you think about it, there's lots of drugs that are out there.

[Prof. Stefanie Jegelka] 20:38:52
They already have to be tested in clinical setting there's a lot of like effort in this FDA approval, and how they are tested, etc.

[Prof. Stefanie Jegelka] 20:39:00
If now, you also want to test the combinations of the drugs, there is a lot of different combinations.

[Prof. Stefanie Jegelka] 20:39:07
If I have a 100 drugs I already have, like hundreds times a 100 combinations more or less, of this drug.

[Prof. Stefanie Jegelka] 20:39:13
That's a large number of drugs. So that's one problem.

[Prof. Stefanie Jegelka] 20:39:20
The other thing is that it doesn't happen for everyone.

[Prof. Stefanie Jegelka] 20:39:22
It just happens for some people. So you'd also need like a sufficient number of patients. So this is actually very difficult.

[Prof. Stefanie Jegelka] 20:39:30
So basically the task that we want to think about now is, I want to predict, I give you a pair of drugs.

[Prof. Stefanie Jegelka] 20:39:37
Can we predict what kinds of side effects may occur if you take those together?

[Prof. Stefanie Jegelka] 20:39:44
So!

[Prof. Stefanie Jegelka] 20:39:48
That's the question. And let's think about how we could actually model it.

[Prof. Stefanie Jegelka] 20:39:52
So this is also a big modeling question, how do you actually set this up?

[Prof. Stefanie Jegelka] 20:40:00
It's a classification problem input is a pair of output.

[Prof. Stefanie Jegelka] 20:40:06
It's a some set of side effects or no side effects, so I have like different ones to choose from.

[Prof. Stefanie Jegelka] 20:40:11
Maybe headaches, stomach ache. Whatnot so that kind of thing so.

[Prof. Stefanie Jegelka] 20:40:19
What could we do to solve this problem? The what kind of information, what kind of data could be used to make that prediction?

[Prof. Stefanie Jegelka] 20:40:32
So here's some suggestions. We could use some like we could use some data on drug drug interactions that we have.

[Prof. Stefanie Jegelka] 20:40:43
But the problem is that these are typically rare. So we don't have a huge lot of data on it.

[Prof. Stefanie Jegelka] 20:40:49
So what else could we use potentially to make our prediction problem a little better in like having, with data available?

[Prof. Stefanie Jegelka] 20:41:01
Having more information available. So this is something. Now, where we can actually be like creative.

[Prof. Stefanie Jegelka] 20:41:08
And we can think about it. What could we use? Maybe I'm sure you have some ideas.

[Prof. Stefanie Jegelka] 20:41:11
What else we could use to to model like on to predict the drug drug interaction.

[Prof. Stefanie Jegelka] 20:41:18
What other data that we use. We can take from some data basis whatever we find.

[Prof. Stefanie Jegelka] 20:41:30
So I'll let you think about it for a few minutes.

[Prof. Stefanie Jegelka] 20:41:47
Okay. I see a few answers already, so drugs and side effects.

[Prof. Stefanie Jegelka] 20:41:55
So individual drugs in their side effects. Yeah, what are the active ingredients?

[Prof. Stefanie Jegelka] 20:41:59
Maybe I have some information about that bio models test in chemical structures that relates to the ingredients.

[Prof. Stefanie Jegelka] 20:42:09
So like some chemical information, maybe molecular information that tells me something about interactions, animal data that would test animals.

[Prof. Stefanie Jegelka] 20:42:23
Some interactions that I know.

[Prof. Stefanie Jegelka] 20:42:27
Create a network with the ingredients of the drug.

[Prof. Stefanie Jegelka] 20:42:33
Then usage.

[Prof. Stefanie Jegelka] 20:42:36
Or do we know some drugs that do not have any interactions?

[Prof. Stefanie Jegelka] 20:42:41
So we have negative examples, allergens demographic data.

[Prof. Stefanie Jegelka] 20:42:45
So we could actually use some other pain data and make this more personalized.

[Prof. Stefanie Jegelka] 20:42:50
That's an idea I could use the Ph that plays the role.

[Prof. Stefanie Jegelka] 20:42:54
Maybe.

[Prof. Stefanie Jegelka] 20:42:58
So we'll use again some demographic information, and so on.

[Prof. Stefanie Jegelka] 20:43:03
Have a controlled population.

[Prof. Stefanie Jegelka] 20:43:08
And look at the doors. Kinetics. Okay? So you have a large number of answers.

[Prof. Stefanie Jegelka] 20:43:14
I really like it, and it ranges from looking at the like molecular structure.

[Prof. Stefanie Jegelka] 20:43:20
The actual molecular dynamics, which are also not trivial to simulate.

[Prof. Stefanie Jegelka] 20:43:24
But you can do it to like looking at demographics of patients, and so on.

[Prof. Stefanie Jegelka] 20:43:30
So this is all great, and that think all of this would greatly improve the model, including all of this information, is actually useful.

[Prof. Stefanie Jegelka] 20:43:39
Let me show you what was done in this study on the paper today.

[Prof. Stefanie Jegelka] 20:43:45
Was on the previous slide. If you look a bit closer.

[Prof. Stefanie Jegelka] 20:43:50
So what they first did they did an exploratory analysis, and they looked at how many drive, what kind of drugs are prescribed together?

[Prof. Stefanie Jegelka] 20:43:58
Typically because maybe I wouldn't like prescribe arbitrary peers.

[Prof. Stefanie Jegelka] 20:44:03
There's some correlation. So and they actually indeed looked at like what kinds of proteins in the body do these drugs act on the drug typically acts on some protein?

[Prof. Stefanie Jegelka] 20:44:14
And that makes it like some reactions. What are these target proteins?

[Prof. Stefanie Jegelka] 20:44:21
And if the drugs are prescribed together, they share like something about them.

[Prof. Stefanie Jegelka] 20:44:27
Pro team, but many of the drugs act on different bodies, so if you take a random pair of graphs, they probably have, like different.

[Prof. Stefanie Jegelka] 20:44:37
So what this means is in general the interactions of the drug and the proteins in the body would actually be useful to include.

[Prof. Stefanie Jegelka] 20:44:47
So we can use that and include that information. And maybe because there are a lot of them don't share a target protein.

[Prof. Stefanie Jegelka] 20:44:54
But there could be an interaction indirectly right.

[Prof. Stefanie Jegelka] 20:44:57
That growth ends interact with other proteins. So there's like some pathway where they intersect.

[Prof. Stefanie Jegelka] 20:45:03
And that is the one that's getting disrupted in something.

[Prof. Stefanie Jegelka] 20:45:06
So we may want to include that as well. And then the other thing is that makes actually life a bit more difficult.

[Prof. Stefanie Jegelka] 20:45:13
Is that different types of side effects occur with different frequency.

[Prof. Stefanie Jegelka] 20:45:18
So some of the side effects occur very frequently, and some very real, and the other thing is that they are not completely independent.

[Prof. Stefanie Jegelka] 20:45:27
So some of them cooler, more than others. So maybe if I have hypertension that cool curs with anxiety, but not necessarily fever. So when you predict the side effects, you don't want to predict them independently, you want to share some information, there.

[Prof. Stefanie Jegelka] 20:45:47
So you wanted to do some joint prediction.

[Prof. Stefanie Jegelka] 20:45:53
Okay, so how could we set this up? And someone already said, Graph, I already, can afford you a graph earlier.

[Prof. Stefanie Jegelka] 20:46:00
So we can build a graph. Now, what does this graph look like?

[Prof. Stefanie Jegelka] 20:46:05
So this is a graph. It's 2 types of notes.

[Prof. Stefanie Jegelka] 20:46:09
It has drug notes, and it has protein nodes.

[Prof. Stefanie Jegelka] 20:46:13
So the dragon notes. Each of the drug notes has some side information.

[Prof. Stefanie Jegelka] 20:46:17
That's like, what type of drug is there chemical information, etc.?

[Prof. Stefanie Jegelka] 20:46:23
And then I have edges, so I have edges between the drugs.

[Prof. Stefanie Jegelka] 20:46:27
This edges have labels, and the label corresponds to the type of side effect, like is this a headache or so? And then I have edges between proteins and drugs.

[Prof. Stefanie Jegelka] 20:46:40
So that means this drug acts on this protein, and it could, of course, be more than one.

[Prof. Stefanie Jegelka] 20:46:45
And then I have interactions between the proteins in the body.

[Prof. Stefanie Jegelka] 20:46:50
So the orange part is really in your body and in the body, like proteins, interact so that could be like some indirect like path from one to the other.

[Prof. Stefanie Jegelka] 20:47:04
So we have.

[Prof. Stefanie Jegelka] 20:47:08
In total. In the study they had 964 types of side effects.

[Prof. Stefanie Jegelka] 20:47:14
That's a lot of different side effects. So you have essentially a multi-class classification problem with 964 classes, plus maybe the class of no side effect.

[Prof. Stefanie Jegelka] 20:47:23
And you have dragon notes and protein notes, more proteins than drugs and lots of edges, different types of edge.

[Prof. Stefanie Jegelka] 20:47:35
So it's a fairly sizable graph.

[Prof. Stefanie Jegelka] 20:47:38
Now, what do we want from this graph? We want a better representation of the drugs.

[Prof. Stefanie Jegelka] 20:47:44
That's more informative about whether they would interact or not.

[Prof. Stefanie Jegelka] 20:47:50
So, and we want to encode. Of course we want to take that network, that into account that we have here that graph.

[Prof. Stefanie Jegelka] 20:47:58
So we will use a graph neural network. And so here's the big picture of what we're doing with we'll take our input graph we'll send it to a graph neural network.

[Prof. Stefanie Jegelka] 20:48:10
So that graph Neal network will compute or learn, and encoding for each node in the graph, especially for the drug rules.

[Prof. Stefanie Jegelka] 20:48:18
And then after that comes to classification part. So this is the classification part it's called the decoder.

[Prof. Stefanie Jegelka] 20:48:25
Also. So this will predict essentially, take the pair of notes, and we'll predict from them the different side effects.

[Prof. Stefanie Jegelka] 20:48:34
So there's basically this is the pair of notes.

[Prof. Stefanie Jegelka] 20:48:39
And what's the probability of having a specific side effect here and the way this works is that it shares the representation.

[Prof. Stefanie Jegelka] 20:48:46
So it actually encodes on the cheers some information about the prediction, so it can in cold correlations and such things.

[Prof. Stefanie Jegelka] 20:48:53
That some side effects are more likely to occur to.

[Prof. Stefanie Jegelka] 20:48:58
But for now I want to focus on the first part the graphical network, and that will use what we just learned.

[Prof. Stefanie Jegelka] 20:49:08
There's only one difference to what I told you are like one variation in that.

[Prof. Stefanie Jegelka] 20:49:13
It. Here we have different.

[Prof. Stefanie Jegelka] 20:49:18
Types of edges. We have a type of edge for each side effect, and we have drug to protein etches. And we have protein protein interact.

[Prof. Stefanie Jegelka] 20:49:29
So we have about 950 types of edge.

[Prof. Stefanie Jegelka] 20:49:36
They are probably not all on the same footing, so whether like a message comes from or an information comes from a protein or a neighbor drug probably makes it different.

[Prof. Stefanie Jegelka] 20:49:48
So maybe I want to process them individually, because maybe there's different things.

[Prof. Stefanie Jegelka] 20:49:53
I want to focus on depending on whether this is a drug neighbor or a protein neighbor.

[Prof. Stefanie Jegelka] 20:50:00
So what we'll do is we're aggregate them separately.

[Prof. Stefanie Jegelka] 20:50:03
So now we will have one aggregation function for each type of edge, and we are just sort of use.

[Prof. Stefanie Jegelka] 20:50:11
That aggregation function for that type of edge and separate out the edges.

[Prof. Stefanie Jegelka] 20:50:20
This is like a picture of it. So what we do is let's say we have this node.

[Prof. Stefanie Jegelka] 20:50:25
See here, see, is this site pro flux in it? Have.

[Prof. Stefanie Jegelka] 20:50:30
One red neighbor to blue neighbors. Here to our 2 neighbors, and then it has 4 protein neighbors.

[Prof. Stefanie Jegelka] 20:50:41
So it has. This was one of the neighbors, that is, the R.

[Prof. Stefanie Jegelka] 20:50:46
One neighbor. That's one of the drug neighbors. So we do one aggregation for that.

[Prof. Stefanie Jegelka] 20:50:49
So what the aggregation here does is it? Does a multiplication by a matrix that's like a simplyified version of our new network.

[Prof. Stefanie Jegelka] 20:50:58
Often neighbor, and then it combines it with the embedding of that Hv.

[Prof. Stefanie Jegelka] 20:51:08
In the end.

[Prof. Stefanie Jegelka] 20:51:10
So!

[Prof. Stefanie Jegelka] 20:51:14
We do this aggregation for each of the neighbors individually, and then we combine them.

[Prof. Stefanie Jegelka] 20:51:19
Then we just sum them up. So this R is basically the summing over the different types of neighbors.

[Prof. Stefanie Jegelka] 20:51:26
And we learn a different weight for each type of neighbor. Different.

[Prof. Stefanie Jegelka] 20:51:32
Otherwise it's really the same as our aggregation function.

[Prof. Stefanie Jegelka] 20:51:38
Okay, so.

[Prof. Stefanie Jegelka] 20:51:43
That's the graph Neural network. How do we train it?

[Prof. Stefanie Jegelka] 20:51:47
What are the training data points here? So now, our what we want this, we have this query, 2 drivers.

[Prof. Stefanie Jegelka] 20:51:54
Would there be a specific side effect? So the training data is basically queries you and V are different drugs and are a specific relation like, does it?

[Prof. Stefanie Jegelka] 20:52:08
What you expect. That particular side effect are as the type of side effect for the drugs you were.

[Prof. Stefanie Jegelka] 20:52:16
And then we trained it like on the here's where we have information.

[Prof. Stefanie Jegelka] 20:52:21
Now, the other thing is because the side effects are rare. We have so-called label and balance.

[Prof. Stefanie Jegelka] 20:52:27
We have many more notes than yes, we have many more examples where this side effect doesn't occur.

[Prof. Stefanie Jegelka] 20:52:33
We don't know whether it's there. Then we have to.

[Prof. Stefanie Jegelka] 20:52:36
Yeah, so what we do is that often a problem. We talked about it a bit in the last lecture we try to balance it out by throwing away some of the no examples.

[Prof. Stefanie Jegelka] 20:52:46
So basically, some of the 0 training data points. And then we train it with stochastic gradient descent.

[Prof. Stefanie Jegelka] 20:52:56
Now, after we've done this, I want to spend a few words on the evaluation of this model.

[Prof. Stefanie Jegelka] 20:53:06
How could you verify that this model works? But what are things you could do?

[Prof. Stefanie Jegelka] 20:53:15
What would you do? Well, how could you verify that this is doing a reasonable job?

[Prof. Stefanie Jegelka] 20:53:52
Okay, I see cross validation and test data.

[Prof. Stefanie Jegelka] 20:53:57
Okay, yeah. So you go to hold out data. And you could to the test error validation.

[Prof. Stefanie Jegelka] 20:54:06
You would try to. If one you'd want to tune the hyper parameters of the model you'd use today.

[Prof. Stefanie Jegelka] 20:54:13
In vitro testing. So you could actually have the model predict, make predictions about site effects that we don't know about.

[Prof. Stefanie Jegelka] 20:54:22
And then you could try it out so you could run a clinical study.

[Prof. Stefanie Jegelka] 20:54:30
Yeah, we could use other like precision, accuracy, etc.

[Prof. Stefanie Jegelka] 20:54:36
Right? So this is all things you could do so actually, because this is something where we make a prediction about this real world data.

[Prof. Stefanie Jegelka] 20:54:44
And part of the data is unknown. That's the challenge about it.

[Prof. Stefanie Jegelka] 20:54:49
So we can do more than just accuracy. So the first thing, if we would run a test that we could see, how well does it predict on the hold our cast that where we know the labels?

[Prof. Stefanie Jegelka] 20:55:00
Then another thing to look at is what I also briefly discussed in the last lecture is, where does it work?

[Prof. Stefanie Jegelka] 20:55:05
Well, and where does it not work well? Are these specific types of predictions or interactions that it's very good at capturing?

[Prof. Stefanie Jegelka] 20:55:13
And are there some that it's not good at capturing so that tells you something about very confusing.

[Prof. Stefanie Jegelka] 20:55:20
So here it works especially well for those where you actually indeed have these 14 interactions.

[Prof. Stefanie Jegelka] 20:55:25
Not so surprisingly, because that's that's basically what the model was.

[Prof. Stefanie Jegelka] 20:55:31
What's encoded in the model.

[Prof. Stefanie Jegelka] 20:55:35
And then another one. Is this literature based evaluation?

[Prof. Stefanie Jegelka] 20:55:41
So that's their replacement of the clinical study.

[Prof. Stefanie Jegelka] 20:55:44
So you said, Okay, well, maybe we can have it. Predict something about a side effect.

[Prof. Stefanie Jegelka] 20:55:50
That's not in the data. And we can see, does it actually exist in reality?

[Prof. Stefanie Jegelka] 20:55:54
So, instead of running the clinical study, what they did is this paper was that they talk.

[Prof. Stefanie Jegelka] 20:56:02
They looked in the recent literature that came basically after that database was constructed that they used to train the model.

[Prof. Stefanie Jegelka] 20:56:09
And say, Look, is there evidence of that where the model predicted?

[Prof. Stefanie Jegelka] 20:56:15
Yes, there is like a I predict that would be a side effect potentially.

[Prof. Stefanie Jegelka] 20:56:19
So they look. Is there anything in the literature in the recent literature that told like says there's evidence for that side effect?

[Prof. Stefanie Jegelka] 20:56:26
And they found several examples where this is actually the case.

[Prof. Stefanie Jegelka] 20:56:31
So these are the examples here.

[Prof. Stefanie Jegelka] 20:56:35
And so this is just to show you that there's like 9 levels at which you can evaluate such models, and that you can better understand how it works and where it works.

[Prof. Stefanie Jegelka] 20:56:44
And admittedly, of course, like you could do the actual like clinical lab studies.

[Prof. Stefanie Jegelka] 20:56:48
In some cases you could do it. That's the most expensive, but that tells you something about how good is the model at predicting things that are not in your data.

[Prof. Stefanie Jegelka] 20:56:56
So that can be very useful, so that brings me to my summary, and then I'll go and like answer some more of the questions.

[Prof. Stefanie Jegelka] 20:57:09
So I showed you like a little tour through neural networks and graph neural networks.

[Prof. Stefanie Jegelka] 20:57:16
Graphs, are you just data type with lots of applications.

[Prof. Stefanie Jegelka] 20:57:20
And what graph neural network essentially do is they find an encoding for the nodes in the graph that captures the neighborhood.

[Prof. Stefanie Jegelka] 20:57:28
Information, and then they aggregate that information into a graph information, if need so there's a lot of examples where they have been used successfully.

[Prof. Stefanie Jegelka] 20:57:40
So it's a good model to learn about.

[Prof. Stefanie Jegelka] 20:57:46
Let me make one more comment about this. So someone said, Well, are we not talking about text models?

[Prof. Stefanie Jegelka] 20:57:52
So let me make one comment. So in the graph neural networks we talked about the aggregation function so you can think about the aggregation function.

[Prof. Stefanie Jegelka] 20:58:00
Really, if I have my the vectors of my node neighbors, these are like my vectors, and I'm using them to update the representation of my essentially and the way I'm doing this is that I'm doing this aggregation.

[Prof. Stefanie Jegelka] 20:58:19
So I'm summing over the neighbors in the neighbors.

[Prof. Stefanie Jegelka] 20:58:23
Neighbors of J. If this is Jay, and then I'm basically doing some transferation of the neighbors.

[Prof. Stefanie Jegelka] 20:58:32
So!

[Prof. Stefanie Jegelka] 20:58:38
This is the H is the representation of the neighbor, and then I'm summing over that.

[Prof. Stefanie Jegelka] 20:58:44
So what is sometimes done is that we actually don't just sum over the neighbors.

[Prof. Stefanie Jegelka] 20:58:50
We actually know how important is each neighbor for that note. So we have actually some kind of it's called an attention rate.

[Prof. Stefanie Jegelka] 20:59:00
That tells me how important is that neighbor for updating minority.

[Prof. Stefanie Jegelka] 20:59:06
And this thing itself. It's learned.

[Prof. Stefanie Jegelka] 20:59:15
And this can depend on various factors. So this, this is now an aggregation with attention, and this app is basically it could just be linear or nonlinear.

[Prof. Stefanie Jegelka] 20:59:27
That's here part that you learned the parameters.

[Prof. Stefanie Jegelka] 20:59:31
So now what is done with. So this is something you can do with graphs you can do it with many other types of data.

[Prof. Stefanie Jegelka] 20:59:37
Whenever you could write. Look at this type of data, it's just a set of vectors, input factors, tokens.

[Prof. Stefanie Jegelka] 20:59:46
So, more generally. This can also be used for sequences.

[Prof. Stefanie Jegelka] 20:59:51
So, if you have a sequence of words like the brown fox or something, then each of the words, if has a vector encoding, you can get this, then you can encode or update the encoding of a vert by looking at the other words in the sentence, so for instance, if I wanted to let's

[Prof. Stefanie Jegelka] 21:00:15
say there's a few more words, but I wanted to encode the fox.

[Prof. Stefanie Jegelka] 21:00:19
I am using all of these others as the neighbors. So now I'm encoding Fox as a function of this other 4 guys.

[Prof. Stefanie Jegelka] 21:00:30
Exactly the same. And the way I do this is by other aggregation, with attention exactly.

[Prof. Stefanie Jegelka] 21:00:37
This thing up here. This is how I'm out updating it.

[Prof. Stefanie Jegelka] 21:00:41
How do I get this? So basically, now, I, what I just have to learn is this part which will be.

[Prof. Stefanie Jegelka] 21:00:52
Essentially, in that case, just a linear transformation. So this will be just W. H.

[Prof. Stefanie Jegelka] 21:00:59
I, and then the attention score, if's basically also just learned with matrices.

[Prof. Stefanie Jegelka] 21:01:07
So if this is the for fox, and this is the word good, then this alpha of fox with the would be.

[Prof. Stefanie Jegelka] 21:01:21
My embedding of box transpose some weight matrix.

[Prof. Stefanie Jegelka] 21:01:26
This is the query with the key matrix, and the H, yeah, so this is how I would get the attention.

[Prof. Stefanie Jegelka] 21:01:37
So this is some kind of similarity measure that tells me how important is that token for that, for including this other token.

[Prof. Stefanie Jegelka] 21:01:46
So this is really the same as an aggregation function, and this is at the core of the transformer architecture.

[Prof. Stefanie Jegelka] 21:01:51
That's essentially what it's doing. And then you'll do some normalization of these attention scores.

[Prof. Stefanie Jegelka] 21:01:59
So this alpha is the normalized version of this thing.

[Prof. Stefanie Jegelka] 21:02:04
So that's essentially what it is. You can here, though you're forgetting the sequence information.

[Prof. Stefanie Jegelka] 21:02:11
So if you want to go back to the sequence information, you do what's called a position on a code, and you essentially encode, add something to the description of the word that encodes where in the sentence it occurred, and otherwise it's the same idea.

[Prof. Stefanie Jegelka] 21:02:26
We have this token, wise processing that use using graphs to use in sets, you using set of words, sequences of words, etc.

[Prof. Stefanie Jegelka] 21:02:35
It's basically all very relate.

[Prof. Stefanie Jegelka] 21:02:38
Okay, so I want you to accept this because that question came up.

[Prof. Stefanie Jegelka] 21:02:43
Now let me go back and answer some of the questions that we haven't answered yet.

[Prof. Stefanie Jegelka] 21:02:51
Okay. Okay.

[Prof. Stefanie Jegelka] 21:02:57
Yeah, so page rank, that question has been answered. You can represent page rank as passing as well.

[Prof. Stefanie Jegelka] 21:03:04
It's like a matrix multiplication, and it can be represented as message.

[Prof. Stefanie Jegelka] 21:03:09
Passing as we have talked about here.

[Prof. Stefanie Jegelka] 21:03:16
Okay.

[Prof. Stefanie Jegelka] 21:03:20
Since real work data for such drug drug interaction outcomes are rare.

[Prof. Stefanie Jegelka] 21:03:24
How can we assert with a reasonable degree of confidence, that our predictions are valid?

[Prof. Stefanie Jegelka] 21:03:31
So for that you could have some holdup data. You need enough data that at least you can learn something that's why you need like you need to source widely and you can try to do the prediction and see like there's evidence in the literature.

[Prof. Stefanie Jegelka] 21:03:48
But of course you want to have enough example that this is quantitative and not just a single pointer.

[Prof. Stefanie Jegelka] 21:03:56
So yeah, that's a big challenge. The evaluation is a big challenge.

[Prof. Stefanie Jegelka] 21:03:58
If you have little data, that's true.

[Prof. Stefanie Jegelka] 21:04:07
Okay, this is all the answers.

[Prof. Stefanie Jegelka] 21:04:23
Are the initial embeddings of the notes chosen randomly, so the initial embedding, if there's variation.

[Prof. Stefanie Jegelka] 21:04:32
So if you, for instance, have a graph where you do have no descriptions like you, think of the molecule, you have some description of the atoms you think of the one with a drugs.

[Prof. Stefanie Jegelka] 21:04:39
You have some feature vector of the drugs you would use that as the initial encoding and the base of the network.

[Prof. Stefanie Jegelka] 21:04:47
In the beginning. When you start training a random, if your graph doesn't have that, it's really just a graph.

[Prof. Stefanie Jegelka] 21:04:54
But you don't have any information about the notes. Then you can use random vectors, and then it's beneficial to use random ones.

[Prof. Stefanie Jegelka] 21:05:01
Or just constant ones. But that's what's typically done very often, though you do have some information on the notes.

[Prof. Stefanie Jegelka] 21:05:09
Like the drug information, the user information, etcetera. You can use that.

[Prof. Stefanie Jegelka] 21:05:20
Okay.

[Prof. Stefanie Jegelka] 21:05:32
Yeah, I think.

[Prof. Stefanie Jegelka] 21:05:35
Answered the question. Maybe help me find more questions that I haven't asked.

[Prof. Stefanie Jegelka] 21:05:42
Maybe there's more in the meanwhile, at the bottom.

[[GL Mentor] Shubham Sharma] 21:05:45
Right? So there is a question from Johnny.

[Prof. Stefanie Jegelka] 21:05:46
Yeah.

[[GL Mentor] Shubham Sharma] 21:05:50
How do we decide? B, C, and D is kept in this second layer?

[Prof. Stefanie Jegelka] 21:05:55
Yeah, this was, I think, the one here. My!

[Prof. Stefanie Jegelka] 21:06:00
Go back to it.

[Prof. Stefanie Jegelka] 21:06:06
Here. So the reason we have Pcnd here in this this year it's called Layer one.

[Prof. Stefanie Jegelka] 21:06:14
But basically these ones here. The question is, why are they sitting here?

[Prof. Stefanie Jegelka] 21:06:18
Why are they feeding into a. So the reason why they are feeding into a is that these are the neighbors of a.

[Prof. Stefanie Jegelka] 21:06:24
In my input, graph this is my input graph and their a has a neighbor.

[Prof. Stefanie Jegelka] 21:06:28
B, neighbor, C, and Neighbor D, so it gets input from neighbors, A, B, C and D, and that goes into its aggregation function.

[Prof. Stefanie Jegelka] 21:06:36
So this is exactly what's kind of depicted here, and if it was also connected to E, then E would also be hanging down here as well.

[[GL Mentor] Shubham Sharma] 21:06:45
Bye, and I guess a related question from Nicholas is that how do we choose the node to start the aggregation and combination?

[[GL Mentor] Shubham Sharma] 21:06:53
Is it random? Or you choose like a node with fewer edges, or something like that?

[Prof. Stefanie Jegelka] 21:06:59
Yeah, it doesn't really matter, because we we go in rounds and in each route, we only update based on the state of the previous round.

[Prof. Stefanie Jegelka] 21:07:09
So it doesn't really matter where you like. Start.

[Prof. Stefanie Jegelka] 21:07:14
And in principle you can vote in parallel as well.

[[GL Mentor] Shubham Sharma] 21:07:21
Are the iterations in gnms, also called epox.

[Prof. Stefanie Jegelka] 21:07:25
Yeah, they are not called epochs. They are just the like Tian and iteration are layers.

[Prof. Stefanie Jegelka] 21:07:34
Epoch is typically referred to in to stochastic, radiant descent, and then, because I'm stochastic, gradient descent, we pick the data points randomly to update our model.

[Prof. Stefanie Jegelka] 21:07:45
What is actually done in practice is, we scramble them, and then we go through them, and then we ramble them and we go through them and bumps going through the data. It's called an epoch.

[Prof. Stefanie Jegelka] 21:07:54
So I've seen each data point once in that epoch, and then like, if I want to have a batch size of one, I would have number of data points, many iterations in this default, but that epoch really refers to going once through the data, and like that sort of orthogonal to

[Prof. Stefanie Jegelka] 21:08:10
like the layers on the network.

[[GL Mentor] Shubham Sharma] 21:08:16
Shayton has a question. Are there standardized formats of pre-trained encoders such that optimizing weights is programmatically feasible.

[Prof. Stefanie Jegelka] 21:08:27
Yeah. So there's there's standard like host websites where you can download them.

[Prof. Stefanie Jegelka] 21:08:35
And they're compatible with like standard toolboxes.

[Prof. Stefanie Jegelka] 21:08:40
Oh, yes! So the pre-trained image, net features, or, like hugging faith, has transformers, etcetera.

[Prof. Stefanie Jegelka] 21:08:47
You can download them and like pick it. It's compatible.

[Prof. Stefanie Jegelka] 21:08:50
With like updating rates, etc.

[Prof. Stefanie Jegelka] 21:08:57
Or like using them as encoding, and then just building on top of them.

[[GL Mentor] Shubham Sharma] 21:09:06
So Amos has a question. So by fixing and transferring, we are just prescribing the initial initialization rates.

[Prof. Stefanie Jegelka] 21:09:17
That's right. So if we fix entrance, yeah, if we just do the fixed and then like, sort of the so called linear readout, we don't even update the weights.

[Prof. Stefanie Jegelka] 21:09:27
We really just create the classifier at the end or we could update the weights.

[Prof. Stefanie Jegelka] 21:09:32
And then again, we could see, that's a really really good initialization.

[Prof. Stefanie Jegelka] 21:09:39
So you only need a few steps to get like to a little bit better.

[Prof. Stefanie Jegelka] 21:09:42
So, but it's already pretty good to start from.

[[GL Mentor] Shubham Sharma] 21:09:50
Could you reinforce the relationship between gnms, transformers, and otherms?

[Prof. Stefanie Jegelka] 21:09:56
Yeah, so are a very different architecture. So the relation between gnms and transformers is the following.

[Prof. Stefanie Jegelka] 21:10:06
So in Gnms we have the aggregation function.

[Prof. Stefanie Jegelka] 21:10:13
So right. What it was doing was, I'm updating.

[Prof. Stefanie Jegelka] 21:10:16
This note here as a function of its neighbors. So down here are the neighbors.

[Prof. Stefanie Jegelka] 21:10:22
This is the node, and then what I'm doing is I'm doing essentially a transformation of the neighbors, and then I'm just ping it and updating my guy from here.

[Prof. Stefanie Jegelka] 21:10:31
So what you sometimes also can do is you actually have a measure of how important is each neighbor that's called the attention rate.

[Prof. Stefanie Jegelka] 21:10:41
So here I'm having that sound with like a I'll I'll talk.

[Prof. Stefanie Jegelka] 21:10:47
Jay is, how is important is note I. To no check.

[Prof. Stefanie Jegelka] 21:10:51
This will to be a modified version of my aggregation function.

[Prof. Stefanie Jegelka] 21:10:55
This also is used in Gnn.

[Prof. Stefanie Jegelka] 21:11:01
So now, what is the transformer doing? A transformer, a model for sequences?

[Prof. Stefanie Jegelka] 21:11:08
It's actually a model for sets. To be honest, the transformer is actually a model that takes a set of vectors.

[Prof. Stefanie Jegelka] 21:11:15
And it output. It updates their representation. That's what it's actually doing.

[Prof. Stefanie Jegelka] 21:11:21
But it can be used for sequences. So let's say that we have a set of vectors, and what we actually want is we want to update all of their representations.

[Prof. Stefanie Jegelka] 21:11:31
And the way we do this is that let's just look at one vector.

[Prof. Stefanie Jegelka] 21:11:35
Let's maybe look at number 2. So we update number 2 as a function of the other vectors.

[Prof. Stefanie Jegelka] 21:11:44
And now this is exactly like this aggregation function.

[Prof. Stefanie Jegelka] 21:11:49
We update number 2 by taking away the neighbors? Is everyone else.

[Prof. Stefanie Jegelka] 21:11:59
So we do like the some of the neighbors, some of all the I called them.

[Prof. Stefanie Jegelka] 21:12:05
I let's keep by all the other guys in my, and then I do.

[Prof. Stefanie Jegelka] 21:12:12
Alpha to I, because this was number 2, and then we do some transaction of the neighbors.

[Prof. Stefanie Jegelka] 21:12:20
So!

[Prof. Stefanie Jegelka] 21:12:23
Transformation is, let's call this transformation. Just.

[Prof. Stefanie Jegelka] 21:12:31
This is the update. And now this is my attention. Right here.

[Prof. Stefanie Jegelka] 21:12:38
And this is my neighbor, representing your neighbor is everyone else.

[Prof. Stefanie Jegelka] 21:12:45
So this is exactly the same that we did for the Gn.

[Prof. Stefanie Jegelka] 21:12:51
And it has that attention rate so there's 2 things we have to decide.

[Prof. Stefanie Jegelka] 21:12:54
What's the neighbor representation update? And the attention.

[Prof. Stefanie Jegelka] 21:12:59
So in this case, this is linear. So we do the labor representation.

[Prof. Stefanie Jegelka] 21:13:03
It's just a matrix times that neighbor and.

[Prof. Stefanie Jegelka] 21:13:11
Then this attention. So this attend actually depends on the representation of this guy, and it depends on the representation of the neighbor.

[Prof. Stefanie Jegelka] 21:13:19
How compatible are they so compatibility is often done with inner product.

[Prof. Stefanie Jegelka] 21:13:25
Similarity is done with inner product. So here we don't exactly do inner product.

[Prof. Stefanie Jegelka] 21:13:31
We do in a product after a linear transformation. So we do.

[Prof. Stefanie Jegelka] 21:13:34
W. Query 2? Is there? We report in this case? H. 2.

[Prof. Stefanie Jegelka] 21:13:40
This is the weary, and then we have the key. So this is W.

[Prof. Stefanie Jegelka] 21:13:44
Key. H. I. This is the key, and then what we do is we take the inner product of those 2 and that's my attention.

[Prof. Stefanie Jegelka] 21:13:52
Wait. So this would be H. 2. Transpose Wq.

[Prof. Stefanie Jegelka] 21:13:57
Transpose Wk without the transport.

[Prof. Stefanie Jegelka] 21:14:03
And h i so this is sort of a score of how compatible they are, and what I then do is I normalize the scores of all the neighbors for 2 to half them between 0 and one.

[Prof. Stefanie Jegelka] 21:14:18
So I do this soft Mac, the same thing that we do when we do when we do magic, request classification.

[Prof. Stefanie Jegelka] 21:14:25
So we do a softmax with of this overall the neighbors.

[Prof. Stefanie Jegelka] 21:14:32
So this is now an architecture that basically just updates the representation of a token as a function of its neighbors.

[Prof. Stefanie Jegelka] 21:14:40
By doing this aggregation operation. It's the same very thing that we do in the Gnn.

[Prof. Stefanie Jegelka] 21:14:47
In, the, gns. We sometimes don't do with attention, sometimes with a attention, and you can infect with this transformer for a graph as well, and if the attention is related to the weight matrix, it's basically the message path.

[Prof. Stefanie Jegelka] 21:15:00
So what do we do when we have a sequence? So that was the sequence.

[Prof. Stefanie Jegelka] 21:15:06
Models here, I said, we have a set of these vectors like neighbors.

[Prof. Stefanie Jegelka] 21:15:10
If we have text, we actually have a sequence. So then, each of these corresponds to a word like the whatever ball it's rolling or something.

[Prof. Stefanie Jegelka] 21:15:21
Each of these vectors corresponds to a word in that case we want to remember, where in the sentence the word is occurring because it matters.

[Prof. Stefanie Jegelka] 21:15:32
If you say the man kicked the ball or the ball kicked the man, it's a bit different, right?

[Prof. Stefanie Jegelka] 21:15:38
So you have to like. Keep the thick vensel to do that.

[Prof. Stefanie Jegelka] 21:15:43
There's a way you can encode the position. So this is typically done with some kind of sign functions that basically tell you you do a few of them that gives you basically the it gives you a short vector that you're uniquely includes the position of that word in the sequence and you just add it.

[Prof. Stefanie Jegelka] 21:16:03
On to your token, encoding essentially, and then you use that as well.

[Prof. Stefanie Jegelka] 21:16:07
So this way you can also focus on your neighbors. That's the main idea.

[Prof. Stefanie Jegelka] 21:16:11
So you see the main idea of this aggregation over like I said of items is very general.

[Prof. Stefanie Jegelka] 21:16:19
It can be used for images. In this case. These tokens are patches of your image, so you can do this over patches and image.

[Prof. Stefanie Jegelka] 21:16:27
You can go over it. You can do it over neighbors in the row.

[Prof. Stefanie Jegelka] 21:16:30
Now someone asked about our Rnn. Specifically for sequences.

[Prof. Stefanie Jegelka] 21:16:37
So this one here is the transformer.

[Prof. Stefanie Jegelka] 21:16:41
And the transformer is really for anytime I have a set, I can add, like some position information, but it's generic.

[Prof. Stefanie Jegelka] 21:16:50
Our end needs a sequence. So what our end does is it goes over the sequence.

[Prof. Stefanie Jegelka] 21:16:59
So the ball is rolling. I have to record recordings.

[Prof. Stefanie Jegelka] 21:17:05
And it basically takes this thing as input this is my model, like a little small.

[Prof. Stefanie Jegelka] 21:17:13
This is an Mlp. Type of model and it could do an output for that.

[Prof. Stefanie Jegelka] 21:17:17
But what I actually do is I replicate this model, and I sort of move it over the sequence.

[Prof. Stefanie Jegelka] 21:17:22
And I apply it on each of the words, and I could put an output on each of the words like, maybe a translation or something.

[Prof. Stefanie Jegelka] 21:17:30
I would want to do. But what this R. And end of it.

[Prof. Stefanie Jegelka] 21:17:33
This is the same model. So this box that's make it a red box.

[Prof. Stefanie Jegelka] 21:17:37
This red box is the same. It's replicated.

[Prof. Stefanie Jegelka] 21:17:42
So what do you do with this red box? Get actually input?

[Prof. Stefanie Jegelka] 21:17:46
Not only from the word, but also from the previous position, and that way you sort of you actually have this thing that slides over your sequence.

[Prof. Stefanie Jegelka] 21:17:55
And if we man it can ring it can remember the information from the previous parts.

[Prof. Stefanie Jegelka] 21:18:02
So this is different from a Cnn. It's different from a gn, and all of those other guys go in parallel. This one.

[Prof. Stefanie Jegelka] 21:18:08
Goes in sequence. It can remember information over the sequence.

[Prof. Stefanie Jegelka] 21:18:13
So that's a difference. This Rnn. Is a different type of idea.

[Prof. Stefanie Jegelka] 21:18:20
And that way you could maybe remember, like, how many parentheses did I open?

[Prof. Stefanie Jegelka] 21:18:25
How many should I close to have this event out, or something like that?

[Prof. Stefanie Jegelka] 21:18:28
So this is what the Rnn. Is very good at that kind of, and then there's variations of it to make this memorization basically better.

[Prof. Stefanie Jegelka] 21:18:36
So we have Nstm, etc. But so the main idea is that you have this thing, and you're moving it over to sequence.

[Prof. Stefanie Jegelka] 21:18:43
And you got like you're replicating the model, but importing the information from the previous step. Again.

[Prof. Stefanie Jegelka] 21:18:52
And there's, of course we could give like a few more lectures on each of these.

[Prof. Stefanie Jegelka] 21:18:58
So this is like the short form of it.

[[GL Mentor] Shubham Sharma] 21:19:02
Yeah, right? Okay. So Rajul is asking, is Rnn, like a time series?

[Prof. Stefanie Jegelka] 21:19:11
So, yeah. Rnn, if you, it's you can use it on time.

[Prof. Stefanie Jegelka] 21:19:16
Series, as well. So yeah, it's it's not.

[Prof. Stefanie Jegelka] 21:19:22
I mean, it can output a time series because it can output like an output for every position that is on, so you can use it to process Time series.

[Prof. Stefanie Jegelka] 21:19:31
Any kind of sequence.

[[GL Mentor] Shubham Sharma] 21:19:34
And there's a question on the drug case study that was discussed.

[[GL Mentor] Shubham Sharma] 21:19:40
How can you validate? How can you validate this other than to those people and see what happens?

[Prof. Stefanie Jegelka] 21:19:45
Yeah, so you can basically have a hold out data set. And you do like test error on that or like accuracy, if you like, you would want to do that to actually see how well are you actually doing?

[Prof. Stefanie Jegelka] 21:20:01
Of course, you need enough data for that. So you need enough data to train the model to do this other things you can do is you can also look at like, where does it make errors?

[Prof. Stefanie Jegelka] 21:20:12
Does it make some systematic errors? Does it work specifically well for certain things, or specifically badly for certain things?

[Prof. Stefanie Jegelka] 21:20:19
So that gives you also an idea where you could maybe trust the model work on what kinds of patience, if you had it had included the patient information, on what kinds of drugs maybe it has some or what kinds of interactions, etc.

[Prof. Stefanie Jegelka] 21:20:34
And then the other thing is, you could actually indeed run clinical studies, or you could look in the literature because the data is necessarily incomplete.

[Prof. Stefanie Jegelka] 21:20:43
Any kind of medical biological data is the current state of the knowledge of our knowledge.

[Prof. Stefanie Jegelka] 21:20:50
Maybe there are other interestingions of proteins that we just don't know yet about, and things like that.

[Prof. Stefanie Jegelka] 21:20:55
So that's possible. I mean, this is what we currently know.

[Prof. Stefanie Jegelka] 21:20:57
So you could see, does it predict something that's not yet in my data?

[Prof. Stefanie Jegelka] 21:21:01
May be reasonable, and I'm looking and so you could look at the literature you could actually try it out if I mean, if the circumstances allowed that, and see if the model predicted it is this actually true in reality, so this is what you'd also do for the molecule prediction things.

[Prof. Stefanie Jegelka] 21:21:19
If you predict that a molecule will work as a drug or have a specific reactivity, would not only run on the machine learning model, you would have the machine learning model make the prediction give you a few good candidates and then you'd actually find them out because that you want to be really short, at this.

[Prof. Stefanie Jegelka] 21:21:35
works, but it helps you like select only a few, so it helps you a lot in not having to do all of them.

[[GL Mentor] Shubham Sharma] 21:21:47
And Danny has a question. Could we analyze baseline side effects from controlled population?

[Prof. Stefanie Jegelka] 21:21:58
You could have, like different populations and like. So that would more be like if you want to understand.

[Prof. Stefanie Jegelka] 21:22:06
Is there like? Do these 2 populations behave differently?

[Prof. Stefanie Jegelka] 21:22:11
So this is something you could also do. This would be like, typically with the control you're trying to get more at like a cost on relationship. Personal back.

[Prof. Stefanie Jegelka] 21:22:20
Yet to controls, and you have to treat it, and then you want to like, see?

[Prof. Stefanie Jegelka] 21:22:24
Is there an actual difference between them? So you could set it up this way as well if you have the data, and if you could actually know what would be the baseline control if you have like, different people or different circumstances, or something like they were they are comparable, but there's like some way, in which that

[Prof. Stefanie Jegelka] 21:22:42
other group differs.

[Prof. Stefanie Jegelka] 21:22:45
Yeah. It's a bit different from the setup here.

[Prof. Stefanie Jegelka] 21:22:48
But in principle you put think about doing something as well.

[Prof. Stefanie Jegelka] 21:22:56
Machine learning is used for inference of these cause of relationships as well.

[[GL Mentor] Shubham Sharma] 21:23:02
Evan has a question. Is there anything that you would suggest?

[[GL Mentor] Shubham Sharma] 21:23:07
Looking more into, or that I could do to look more at Gnn's, potentially combined with hybrid quantum computers on drug modeling.

[Prof. Stefanie Jegelka] 21:23:16
Oh, okay.

[Prof. Stefanie Jegelka] 21:23:19
So I don't have a specific pointer on quantum computers.

[Prof. Stefanie Jegelka] 21:23:31
That I have to pass on. So if you just want to try out a bit of the gnms, I would suggest some of the toolboxes that I, one of those slides at the end of the Gnn.

[Prof. Stefanie Jegelka] 21:23:42
Part, and they have, like tutorials, etc.

[Prof. Stefanie Jegelka] 21:23:46
But this is more, for like, if you just want to get familiar with it like.

[Prof. Stefanie Jegelka] 21:23:48
So the introductory code, examples, etc. But this is not independent of what this is just.

[Prof. Stefanie Jegelka] 21:23:55
Gnn.

[[GL Mentor] Shubham Sharma] 21:23:57
Right? And what someone also asked earlier, what's a good tool for deployment, you know, across as your Aws Google, what would be your preference?

[Prof. Stefanie Jegelka] 21:24:14
So!

[Prof. Stefanie Jegelka] 21:24:18
Could be anything. One thing is that, like all other neural networks, are put with gpus, like the matrix, because the all of it is essentially matrix multiplication gnms are not as good with Gpus because of the irregularity of the neighborhood sizes so they actually are typically

[Prof. Stefanie Jegelka] 21:24:38
runoffs. I mean you can, but like they don't gain as much as the others.

[Prof. Stefanie Jegelka] 21:24:42
You can run on the seat. So that's like one thing, because you have this aggregation operation.

[Prof. Stefanie Jegelka] 21:24:49
But then, like the neighborhoods, are different. So it's like a little bit less regular than the other neural networks. So it leads, like some special like thoughts on scaling it, etcetera.

[[GL Mentor] Shubham Sharma] 21:25:04
And just to add to that, I have work on Aws.

[[GL Mentor] Shubham Sharma] 21:25:08
So I do extensive work on aws, and what I see in aws is there are a lot of, you know, sort of cookbooks right readymade notebooks on a lot of different applications.

[[GL Mentor] Shubham Sharma] 21:25:24
They have, a service known as Amazon, Neptune, which all about graphical neural networks and graph-based applications, and they have a lot of applications and fully fully populated notebooks for different applications and graphs as well like you know, link prediction node regression node

[[GL Mentor] Shubham Sharma] 21:25:43
classification, these type of things. So you can definitely explore that.

[[GL Mentor] Shubham Sharma] 21:25:47
So next question is, what if the aggregation would be an average instead of the sum?

[[GL Mentor] Shubham Sharma] 21:25:54
Would that work the same?

[Prof. Stefanie Jegelka] 21:25:56
It would work the same. It would work the same pretty much.

[Prof. Stefanie Jegelka] 21:26:02
There's some differences on which one can represent a bit more.

[Prof. Stefanie Jegelka] 21:26:08
So the basically one intuition is that like, if you think about the social network, the average would basically give you sort of statistics of your neighbors, like 10% of your friends.

[Prof. Stefanie Jegelka] 21:26:19
I'm interested in soccer and like 50% are interested. Whatever.

[Prof. Stefanie Jegelka] 21:26:23
Reading, books, if you do the sum, you would actually get the number instead of the sort of the percentage, the fraction you would essentially get the number.

[Prof. Stefanie Jegelka] 21:26:33
And sometimes it's useful to have to have the number. Sometimes the averaging is good because it makes it a little bit less dependent on the number of neighbors, so which one works better.

[Prof. Stefanie Jegelka] 21:26:44
It's a little bit application dependent. But the rest of it is pretty much the same.

[Prof. Stefanie Jegelka] 21:26:49
So it's basically you just divide by the number of neighbors.

[Prof. Stefanie Jegelka] 21:26:52
It's an easy operation, the rest is the same.

[[GL Mentor] Shubham Sharma] 21:26:58
So Neha has a question in essence, the node is selected by doing depth.

[[GL Mentor] Shubham Sharma] 21:27:04
First search to find the first layer to work on.

[Prof. Stefanie Jegelka] 21:27:08
So it depends what you mean by finding the note so.

[Prof. Stefanie Jegelka] 21:27:15
You. What you do is you do basically just the you do all the notes in parallel.

[Prof. Stefanie Jegelka] 21:27:21
So and you do this, let me go back to this one here.

[Prof. Stefanie Jegelka] 21:27:23
So in each round you, for each note you look at its neighbors, and you update and that you then you have.

[Prof. Stefanie Jegelka] 21:27:34
Once you have done this then in the next round again for each node you look at all their neighbors.

[Prof. Stefanie Jegelka] 21:27:39
Aggregate and update, and so on. So you really do this in parallel.

[Prof. Stefanie Jegelka] 21:27:46
So, yeah, so you're actually doing it. Yeah, it's okay.

[Prof. Stefanie Jegelka] 21:27:52
So you that's for search. Yeah, maybe it's closer to breadth. First search.

[Prof. Stefanie Jegelka] 21:27:57
But it's not actually really a search, because you're doing it like layer by layer. Essentially.

[[GL Mentor] Shubham Sharma] 21:28:03
Alright depth. First search can be useful in finding the shortest path in a graph. But I guess, for finding the first node, or something like that, it's depends on a lot of other things as well as already discussed.

[[GL Mentor] Shubham Sharma] 21:28:18
So it is not clear to me in the case of graphs, how concepts like Resnet or more hidden layers, can be combined, since it is not clear to me concepts like envision, we look for edges and more layers in case of complex shapes where we need

[[GL Mentor] Shubham Sharma] 21:28:35
hierarchical representations. Can you assist me? How do you decide for H.

[[GL Mentor] Shubham Sharma] 21:28:39
0 h. One h. 2 or more layers in a graph.

[Prof. Stefanie Jegelka] 21:28:42
Hi yeah, that's a good one. I didn't actually talk about this.

[Prof. Stefanie Jegelka] 21:28:47
So in. It's really true. If you think about computer vision, we said we had this hierarchy right from edges to shapes, etc.

[Prof. Stefanie Jegelka] 21:28:56
It's very visual, and more layers is better in graph neural networks.

[Prof. Stefanie Jegelka] 21:29:00
That is not necessarily the case, and very often the number of these iterations.

[Prof. Stefanie Jegelka] 21:29:05
Is like between 2 and 5%. And my is that so if you think about this being a social network, for instance, in in each iteration, you get like information from a larger neighborhood.

[Prof. Stefanie Jegelka] 21:29:19
But now let's thank you would actually want to make a prediction about a persons, interests.

[Prof. Stefanie Jegelka] 21:29:27
Social is the 10. Step away, friend, really that important? Probably that's like, if I'm who are your friends, and what other properties?

[Prof. Stefanie Jegelka] 21:29:35
And maybe they are friends. That's already pretty good, and you don't actually need that much more information.

[Prof. Stefanie Jegelka] 21:29:41
All the other information would sort of just be noise or wash out that information.

[Prof. Stefanie Jegelka] 21:29:47
So that's a bit of an intuition by sometimes.

[Prof. Stefanie Jegelka] 21:29:51
You don't want to go too many layers. The actual layers depends, of course, on the application, and, like the structure of the graph, if a graph is very well connected that, like in 2 hops, you have a lot of neighbors, then you may actually want to do fewer.

[Prof. Stefanie Jegelka] 21:30:08
Iterations. If you have like, in a few steps, you have ever only very few, because the graph is very sparsely connected.

[Prof. Stefanie Jegelka] 21:30:14
Maybe you want to do a few more so it depends on that as well and then it depends on the task as well.

[Prof. Stefanie Jegelka] 21:30:20
So if you do this social neighborhood predictions do it usually like smaller neighborhoods, is appropriate, because all you care about is more like some average statistics over your neighbors thought of like a recommendation or something like that.

[Prof. Stefanie Jegelka] 21:30:37
If you do something like finding a path, you need to do enough iterations to sort of numbers. Iteration sort of has to be the length of the tab.

[Prof. Stefanie Jegelka] 21:30:47
So for some applications you have to go deeper if you want to find some structural properties, you may have to go.

[Prof. Stefanie Jegelka] 21:30:53
Enough to actually propagate enough information that you have that like structure in your neighborhood, essentially.

[Prof. Stefanie Jegelka] 21:31:02
So, that it depends a little bit what in your application may be the important thing that this should actually include so that's a bit of the intuition.

[Prof. Stefanie Jegelka] 21:31:10
Now things like resnet, so resonance. Yes, it's for depth, however, in graphs we also use the skip connections.

[Prof. Stefanie Jegelka] 21:31:19
And the reason. One reason, for instance, is that then, in some sense, so you could skip to the like last layer, and then you could say the last layer can easily select how many steps would we actually go?

[Prof. Stefanie Jegelka] 21:31:33
And so you could do like whatever 5 or 10, and then iterations, and then from each iteration you have sort of I call and you can calculate them in the end.

[Prof. Stefanie Jegelka] 21:31:46
So then the final layer can essentially decide for your task, which of them is the most useful?

[Prof. Stefanie Jegelka] 21:31:52
So that way you can use skip connections and graphnets as well, and they do help you actually to go a bit deeper as well.

[[GL Mentor] Shubham Sharma] 21:32:01
Okay, so we are almost running out of time. So let's take one.

[[GL Mentor] Shubham Sharma] 21:32:04
One more question. So Neha has this question, do you have any comments on Google's latest announcement about the generative AI and masterrooms?

[Prof. Stefanie Jegelka] 21:32:15
So I mean, there's been a lot of comments on the advancements in in AI, and I mean talking about all the language amongst the large language models right?

[Prof. Stefanie Jegelka] 21:32:31
Instead of maybe commenting specifically on this, let me just make some general connections to what you learned about today.

[Prof. Stefanie Jegelka] 21:32:39
And like, maybe the state of machine learning. So it's true that sort of in the later, like a recent task, like the large language models have been like.

[Prof. Stefanie Jegelka] 21:32:48
First. It was sort of the computer vision models, and we made them big.

[Prof. Stefanie Jegelka] 21:32:51
And we have the breakthroughs, and then maybe the generative models with like the diffusion, models, etc., like like, etc., all these models, and then the language models all of these models use the pre tracking some sort of pre-training that we've

[Prof. Stefanie Jegelka] 21:33:09
talked about before the models get larger and larger, they get more and more bulky.

[Prof. Stefanie Jegelka] 21:33:12
They're harder and harder to train. This is the problem it's a challenge for us so a lot of this, what is being done is just the fine tuning.

[Prof. Stefanie Jegelka] 21:33:22
And it is a discussion on like, what do we? What do we?

[Prof. Stefanie Jegelka] 21:33:28
Where do we want to go with these models? How would we?

[Prof. Stefanie Jegelka] 21:33:31
Should we think about these models? I mean, there's a big discussions on, I mean how to responsibly use these models.

[Prof. Stefanie Jegelka] 21:33:40
They can do a ton of things. They have a ton of implications.

[Prof. Stefanie Jegelka] 21:33:44
So I talked a tiny bit about the potential like ethical and social implications.

[Prof. Stefanie Jegelka] 21:33:48
In my second lecture at the end there's a huge lot of more to say about the language models.

[Prof. Stefanie Jegelka] 21:33:54
What implications they have on everyone on future, of work, on like people's lives, on credibility, on all these kinds of things.

[Prof. Stefanie Jegelka] 21:34:05
They can do amazing things. They need to be controlled.

[Prof. Stefanie Jegelka] 21:34:09
Other questions that are. There is, of course, who can train these models?

[Prof. Stefanie Jegelka] 21:34:13
With the resources, and I mean there's more and more open source going on which is actually very important.

[Prof. Stefanie Jegelka] 21:34:21
And it's very useful.

[Prof. Stefanie Jegelka] 21:34:24
And but like initially, as a lot of them, not everyone can train these models.

[Prof. Stefanie Jegelka] 21:34:32
This is like now out in the head to like having enough compute.

[Prof. Stefanie Jegelka] 21:34:36
Everyone can try to, and like people are working on it.

[Prof. Stefanie Jegelka] 21:34:39
But there's like a lot of challenges related to this, and there's a lot of discussion needed in on several end on this.

[Prof. Stefanie Jegelka] 21:34:49
Yes, there's more than what I can say like in a few minutes. Here.

[[GL Mentor] Shubham Sharma] 21:34:53
Yeah, the I mean, deep learning is a vast feel, and we were able to get into substantial amount of details in these lectures.

[[GL Mentor] Shubham Sharma] 21:35:01
So it, indeed. It was a great week, covering all the interesting topics, and this domain.

[[GL Mentor] Shubham Sharma] 21:35:07
So thanks a lot, Professor, for that, and that would mark the end of this module. So thank you. Everyone.

[[GL Mentor] Shubham Sharma] 21:35:12
Thanks a lot for the active participation. They were, indeed, some great questions during these sessions, as well.

[[GL Mentor] Shubham Sharma] 21:35:19
So thanks for that. And with that I wish you good luck.

[[GL Mentor] Shubham Sharma] 21:35:24
Bye, and I see you soon.

[Prof. Stefanie Jegelka] 21:35:27
Thank you.

